{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7e6063a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_1 = pd.read_csv('/content/drive/My Drive/Human vs AI Generated Text Classification/1765533433232_96d2a190cc.csv')"
      ],
      "metadata": {
        "id": "goI-oW53raNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1"
      ],
      "metadata": {
        "id": "8XBn0z8BsmjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1.dtypes"
      ],
      "metadata": {
        "id": "5x-sjCst2UTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1.duplicated().sum()"
      ],
      "metadata": {
        "id": "breey0givQ4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no duplicate values in the first dataset."
      ],
      "metadata": {
        "id": "2sgdK4A5veMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_1.isnull().sum()"
      ],
      "metadata": {
        "id": "Q1B-mkI4vk52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "343 null valus in 'notes' column."
      ],
      "metadata": {
        "id": "d-tPv6knv9Yo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df05f18e"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "custom_bins = [0, 50, 100, 150, 200, 250, 300]\n",
        "df_1['length_chars_binned'] = pd.cut(df_1['length_chars'], bins=custom_bins, right=False)\n",
        "binned_counts = df_1['length_chars_binned'].value_counts().sort_index()\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.barplot(x=binned_counts.index.astype(str), y=binned_counts.values, palette='viridis', hue=binned_counts.index.astype(str), legend=False)\n",
        "plt.title('Character length count of df_1')\n",
        "plt.xlabel('Character Length bins')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0, ha='center')\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_height())}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center',\n",
        "                xytext=(0, 9),\n",
        "                textcoords='offset points')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c97d2a5f"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "custom_bins_words = [0, 10, 15, 20, 25, 30, 35, 40, 45]\n",
        "df_1['length_words_binned'] = pd.cut(df_1['length_words'], bins=custom_bins_words, right=False)\n",
        "binned_words_counts = df_1['length_words_binned'].value_counts().sort_index()\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.barplot(x=binned_words_counts.index.astype(str), y=binned_words_counts.values, palette='viridis', hue=binned_words_counts.index.astype(str), legend=False)\n",
        "plt.title('Word count of df_1')\n",
        "plt.xlabel('Word Length Bins')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0, ha='center')\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_height())}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center',\n",
        "                xytext=(0, 9),\n",
        "                textcoords='offset points')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baab5265"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "label_counts = df_1['label'].value_counts()\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=90, colors=sns.color_palette('viridis'))\n",
        "plt.title('Distribution of Labels')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a90cbf66"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "correlation_columns = ['length_chars', 'length_words', 'quality_score', 'sentiment', 'plagiarism_score']\n",
        "df_corr = df_1[correlation_columns]\n",
        "correlation_matrix = df_corr.corr()\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='Blues', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following columns were excluded from model training to prevent data leakage, reduce bias, and ensure that the classifier learned intrinsic linguistic patterns rather than relying on metadata or weak auxiliary signals. The **quality_score** column was dropped because readability-based metrics capture stylistic uniformity but show high overlap between human and AI-generated text, making them weak and non-decisive predictors. Similarly, the **sentiment** column was excluded as emotional tone exhibits very weak correlation with AI authorship and does not reliably distinguish between human and AI writing. Metadata attributes such as **topic** were removed due to the risk of shortcut learning, as topic information can indirectly reveal labels without reflecting authorship style. The **source_detail** column was excluded because it explicitly identifies content origin (e.g., human author IDs or AI model names), which would cause severe data leakage and artificially inflate model performance. The **timestamp** column was dropped as temporal information is irrelevant to linguistic structure and may introduce chronological bias. The **plagiarism** column was excluded because plagiarism indicators are not causally related to AI text generation and are often noisy or inconsistently defined. Finally, the **notes** column was removed due to its subjective and human-annotated nature, which lacks consistency and does not represent intrinsic text characteristics. These columns were retained only for exploratory data analysis, stratified sampling, and bias analysis, while the final model was trained exclusively on text-based features."
      ],
      "metadata": {
        "id": "6_7f3JKfE_3d"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aba68dc5"
      },
      "source": [
        "relevant_columns = ['text', 'label', 'length_chars', 'length_words']\n",
        "df_1_modified = df_1[relevant_columns].copy()\n",
        "df_1_modified"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**punctuation_ratio** measures how much punctuation a text uses relative to its length.\n",
        "\n",
        "Formula: punctuation_ratio = number_of_punctuation_characters / number_of_characters\n",
        "\n",
        "Why this matters:\n",
        "\n",
        "AI-generated text tends to use safe, grammatically “correct” punctuation, avoid expressive or erratic punctuation (!!!, ?!, —, etc.).\n",
        "\n",
        "On the other hand humans are inconsistent, overuse or underuse punctuation, use stylistic punctuation.\n",
        "\n",
        "This makes punctuation usage a useful stylometric signal."
      ],
      "metadata": {
        "id": "JSSp2mWWfcCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**repetition_score** measures how repetitive a text is, i.e., how often words or phrases are reused. AI text repeats patterns more consistently than human text. Human writing may contain similar expressions, phrases and words over and over whereas AI generated texts use more moderated, neural texts and synonyms.\n",
        "\n",
        "Formula: repetition_score = 1 - (unique_words / total_words)\n",
        "\n",
        "High repetition score → fewer unique words → more repetition\n",
        "\n",
        "Low repetition score → more lexical diversity"
      ],
      "metadata": {
        "id": "yDnRLBg1gbZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def punctuation_ratio(text):\n",
        "    if not isinstance(text, str) or len(text) == 0:\n",
        "        return 0.0\n",
        "    punct = sum(1 for c in text if c in string.punctuation)\n",
        "    return punct / len(text)\n",
        "\n",
        "def repetition_score(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 0.0\n",
        "    words = text.lower().split()\n",
        "    if len(words) == 0:\n",
        "        return 0.0\n",
        "    return 1 - len(set(words)) / len(words)"
      ],
      "metadata": {
        "id": "gHEXum0CdWr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1_modified[\"punctuation_ratio\"] = df_1_modified[\"text\"].apply(punctuation_ratio)\n",
        "df_1_modified[\"repetition_score\"] = df_1_modified[\"text\"].apply(repetition_score)"
      ],
      "metadata": {
        "id": "EZ8o73HIdbm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1_modified"
      ],
      "metadata": {
        "id": "tVu2nim3eJU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85a0c261"
      },
      "source": [
        "df_1_modified['label'] = df_1_modified['label'].map({'human': 0, 'ai': 1}).astype(int)\n",
        "df_1_modified.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1_modified.to_csv('/content/drive/My Drive/Human vs AI Generated Text Classification/df_1_modified.csv', index=False)"
      ],
      "metadata": {
        "id": "syuwX6-wxTfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2 = pd.read_csv('/content/drive/My Drive/Human vs AI Generated Text Classification/your_dataset_5000.csv')"
      ],
      "metadata": {
        "id": "BnoCuLu_hLGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2"
      ],
      "metadata": {
        "id": "K4EDbgljiUxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2.info()"
      ],
      "metadata": {
        "id": "NojzrAjXitiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2.isnull().sum()"
      ],
      "metadata": {
        "id": "vD-hOEXYi1fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2.duplicated().sum()"
      ],
      "metadata": {
        "id": "T6tw6n6qjDXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A very large portion of the dataset consists of repeated rows. Total duplicate rows 4540. This means only 460 rows are unique.\n"
      ],
      "metadata": {
        "id": "795dmFYDnJMj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "647c3fa7"
      },
      "source": [
        "df_2.drop_duplicates(inplace=True)\n",
        "print(\"Shape of df_2 after removing duplicates:\", df_2.shape)\n",
        "display(df_2.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2[\"text\"] = df_2[\"text\"].astype(str)\n",
        "df_2[\"length_chars\"] = df_2[\"text\"].str.len()\n",
        "df_2[\"length_words\"] = df_2[\"text\"].str.split().str.len()\n",
        "df_2[\"punctuation_ratio\"] = df_2[\"text\"].apply(punctuation_ratio)\n",
        "df_2[\"repetition_score\"] = df_2[\"text\"].apply(repetition_score)\n",
        "df_2"
      ],
      "metadata": {
        "id": "nS_5oT8pkRh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "434912c7"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "min_len = df_2['length_chars'].min()\n",
        "max_len = df_2['length_chars'].max()\n",
        "\n",
        "# Create custom bins that cover the actual range of data, with an interval of 50\n",
        "custom_bins = np.arange(0, max_len + 50, 50).tolist()\n",
        "\n",
        "df_2['length_chars_binned'] = pd.cut(df_2['length_chars'], bins=custom_bins, right=False)\n",
        "binned_counts = df_2['length_chars_binned'].value_counts().sort_index()\n",
        "\n",
        "# Filter out bins with zero counts\n",
        "binned_counts = binned_counts[binned_counts > 0]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.barplot(x=binned_counts.index.astype(str), y=binned_counts.values, palette='viridis', hue=binned_counts.index.astype(str), legend=False)\n",
        "plt.title('Character length count of df_2')\n",
        "plt.xlabel('Character Length Bins')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0, ha='center')\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_height())}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center',\n",
        "                xytext=(0, 9),\n",
        "                textcoords='offset points')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dc4a34a"
      },
      "source": [
        "df_2_modified = df_2.copy()\n",
        "df_2_modified.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ccc87eb"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "min_len_words = df_2['length_words'].min()\n",
        "max_len_words = df_2['length_words'].max()\n",
        "\n",
        "# Create custom bins that cover the actual range of data, with an interval of 10 for words\n",
        "custom_bins_words = np.arange(0, max_len_words + 10, 10).tolist()\n",
        "\n",
        "df_2['length_words_binned'] = pd.cut(df_2['length_words'], bins=custom_bins_words, right=False)\n",
        "binned_words_counts = df_2['length_words_binned'].value_counts().sort_index()\n",
        "\n",
        "# Filter out bins with zero counts\n",
        "binned_words_counts = binned_words_counts[binned_words_counts > 0]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.barplot(x=binned_words_counts.index.astype(str), y=binned_words_counts.values, palette='viridis', hue=binned_words_counts.index.astype(str), legend=False)\n",
        "plt.title('Word count of df_2')\n",
        "plt.xlabel('Word Length Bins')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0, ha='center')\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_height())}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center',\n",
        "                xytext=(0, 9),\n",
        "                textcoords='offset points')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f532868"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "label_counts_df2 = df_2['label'].value_counts()\n",
        "label_names = {0: 'human', 1: 'ai'}\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(label_counts_df2, labels=label_counts_df2.index.map(label_names), autopct='%1.1f%%', startangle=90, colors=sns.color_palette('viridis'))\n",
        "plt.title('Distribution of Labels in df_2')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "389140c6"
      },
      "source": [
        "df_2_modified = df_2_modified.drop(columns=['length_chars_binned', 'length_words_binned'], errors='ignore')\n",
        "df_2_modified.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c16ede4"
      },
      "source": [
        "df_2_modified.to_csv('/content/drive/My Drive/Human vs AI Generated Text Classification/df_2_modified.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_3 = pd.read_csv('/content/drive/My Drive/Human vs AI Generated Text Classification/AI_Human_balanced_dataset.csv')"
      ],
      "metadata": {
        "id": "9KJ1dPIqndhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_3"
      ],
      "metadata": {
        "id": "z_BcpFsrnxBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_3.info()"
      ],
      "metadata": {
        "id": "GyQPSSRgoFVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_3 = df_3.rename(columns={'generated': 'label'})"
      ],
      "metadata": {
        "id": "iUd0KNepochd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_3"
      ],
      "metadata": {
        "id": "1yiDOIkgovSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "959d3fcc"
      },
      "source": [
        "df_3['label'] = df_3['label'].astype(int)\n",
        "print(\"DataFrame df_3 info after converting 'label' to int:\")\n",
        "df_3.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_3"
      ],
      "metadata": {
        "id": "lEFzd7prpKES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_3.isnull().sum()"
      ],
      "metadata": {
        "id": "qe9cZN7SpVJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_3.duplicated().sum()"
      ],
      "metadata": {
        "id": "kE8JyKTypZj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "df_3 is too large to handle and time consuming. So, 20k entries from df_3 (10k human and 10k AI generated) is taken before proceeding. It is named as df_3_truncated."
      ],
      "metadata": {
        "id": "rBumVJrF1fW6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9ee461a"
      },
      "source": [
        "# Separate df_3 into two dataframes based on label\n",
        "df_3_label_0 = df_3[df_3['label'] == 0]\n",
        "df_3_label_1 = df_3[df_3['label'] == 1]\n",
        "\n",
        "# Sample 10,000 entries from each label\n",
        "sample_size = 10000\n",
        "df_3_sampled_0 = df_3_label_0.sample(n=min(len(df_3_label_0), sample_size), random_state=42)\n",
        "df_3_sampled_1 = df_3_label_1.sample(n=min(len(df_3_label_1), sample_size), random_state=42)\n",
        "\n",
        "# Concatenate the sampled dataframes to create df_3_truncated\n",
        "df_3_truncated = pd.concat([df_3_sampled_0, df_3_sampled_1])\n",
        "\n",
        "# Shuffle the new dataframe to mix the labels\n",
        "df_3_truncated = df_3_truncated.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Shape of df_3_truncated:\", df_3_truncated.shape)\n",
        "display(df_3_truncated.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence aware truncation:\n",
        "\n",
        "This is to ensure maximum text length is 512 tokens and there are no broken sententence in text column."
      ],
      "metadata": {
        "id": "7GKIOkQcbqwP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5541d21d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45545522"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f2fac37"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI vs hman text classification relies mostly on writing style, repetation, sentence structure, lexical and error patterns (humans make incosistent expressions). In practice the first 200 - 400 tokens already contain enough signal. The rest is often redundant stylistically. So, 512 tokens are often enough for text classification and hence sentence aware text truncation does not destroy the work."
      ],
      "metadata": {
        "id": "UHaJe-QrONmv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "591db38a"
      },
      "source": [
        "import string\n",
        "import nltk\n",
        "\n",
        "def truncate_text(text, max_tokens=512):\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    truncated = []\n",
        "    total_tokens = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        # Encode sentence WITHOUT special tokens\n",
        "        sent_tokens = len(\n",
        "            tokenizer.encode(sent, add_special_tokens=False)\n",
        "        )\n",
        "\n",
        "        if total_tokens + sent_tokens > max_tokens:\n",
        "            break\n",
        "\n",
        "        truncated.append(sent)\n",
        "        total_tokens += sent_tokens\n",
        "\n",
        "    return \" \".join(truncated).strip()\n",
        "\n",
        "df_3_truncated['text'] = df_3_truncated['text'].apply(truncate_text)\n",
        "display(df_3_truncated.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_3_truncated[\"text\"] = df_3_truncated[\"text\"].astype(str)\n",
        "df_3_truncated[\"length_chars\"] = df_3_truncated[\"text\"].str.len()\n",
        "df_3_truncated[\"length_words\"] = df_3_truncated[\"text\"].str.split().str.len()\n",
        "df_3_truncated[\"punctuation_ratio\"] = df_3_truncated[\"text\"].apply(punctuation_ratio)\n",
        "df_3_truncated[\"repetition_score\"] = df_3_truncated[\"text\"].apply(repetition_score)\n",
        "df_3_truncated"
      ],
      "metadata": {
        "id": "4dALospI_HXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4b13288"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "min_len = df_3_truncated['length_chars'].min()\n",
        "max_len = df_3_truncated['length_chars'].max()\n",
        "\n",
        "# Create custom bins that cover the actual range of data, with an interval of 50\n",
        "custom_bins = np.arange(0, max_len + 50, 500).tolist()\n",
        "\n",
        "df_3_truncated['length_chars_binned'] = pd.cut(df_3_truncated['length_chars'], bins=custom_bins, right=False)\n",
        "binned_counts = df_3_truncated['length_chars_binned'].value_counts().sort_index()\n",
        "\n",
        "# Filter out bins with zero counts\n",
        "binned_counts = binned_counts[binned_counts > 0]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.barplot(x=binned_counts.index.astype(str), y=binned_counts.values, palette='viridis', hue=binned_counts.index.astype(str), legend=False)\n",
        "plt.title('Character length count of df_3_truncated')\n",
        "plt.xlabel('Character Length Bins')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0, ha='center')\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_height())}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center',\n",
        "                xytext=(0, 9),\n",
        "                textcoords='offset points')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b73b6010"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "min_len_words = df_3_truncated['length_words'].min()\n",
        "max_len_words = df_3_truncated['length_words'].max()\n",
        "\n",
        "# Create custom bins that cover the actual range of data, with an interval of 10 for words\n",
        "custom_bins_words = np.arange(0, max_len_words + 10, 50).tolist()\n",
        "\n",
        "df_3_truncated['length_words_binned'] = pd.cut(df_3_truncated['length_words'], bins=custom_bins_words, right=False)\n",
        "binned_words_counts = df_3_truncated['length_words_binned'].value_counts().sort_index()\n",
        "\n",
        "# Filter out bins with zero counts\n",
        "binned_words_counts = binned_words_counts[binned_words_counts > 0]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.barplot(x=binned_words_counts.index.astype(str), y=binned_words_counts.values, palette='viridis', hue=binned_words_counts.index.astype(str), legend=False)\n",
        "plt.title('Word count of df_3_truncated')\n",
        "plt.xlabel('Word Length Bins')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0, ha='center')\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_height())}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center',\n",
        "                xytext=(0, 9),\n",
        "                textcoords='offset points')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22d44b06"
      },
      "source": [
        "df_3_truncated.to_csv('/content/drive/My Drive/Human vs AI Generated Text Classification/df_3_truncated.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6bec7ad"
      },
      "source": [
        "df_combined = pd.concat([df_1_modified, df_2_modified, df_3_truncated], ignore_index=True)\n",
        "df_combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8dd0f83"
      },
      "source": [
        "df_combined = df_combined.drop(columns=['length_chars_binned', 'length_words_binned'])\n",
        "df_combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined.duplicated().sum()"
      ],
      "metadata": {
        "id": "cIbS5jTKhFDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f56ff534"
      },
      "source": [
        "df_combined.drop_duplicates(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0804b84"
      },
      "source": [
        "print(\"Shape of df_combined after removing duplicates:\", df_combined.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "271ec072"
      },
      "source": [
        "df_combined.to_csv('/content/drive/My Drive/Human vs AI Generated Text Classification/df_combined.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s.,!?']\", ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df_combined[\"text\"] = df_combined[\"text\"].apply(clean_text)"
      ],
      "metadata": {
        "id": "b1NGm8Jph_dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step normalizes the raw text to make it suitable for TF-IDF feature extraction. The **clean_text()** function converts all text to lowercase, removes punctuation and other non-alphanumeric characters using regular expressions, and standardizes spacing by collapsing multiple spaces into one and trimming leading or trailing whitespace."
      ],
      "metadata": {
        "id": "pKubkyPsjY1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined"
      ],
      "metadata": {
        "id": "AknpziNYj9w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization using spaCy is not strictly necessary in all text classification tasks, but it can be very beneficial, especially for distinguishing between human and AI-generated text.\n",
        "\n",
        "Here's why it's generally a good idea for this type of task:\n",
        "\n",
        "Reduces Dimensionality:\n",
        "\n",
        "It reduces words to their base or dictionary form (lemma). For example, 'running', 'runs', and 'ran' all become 'run'. This reduces the total number of unique tokens in vocabulary, which can simplify model and prevent overfitting.\n",
        "\n",
        "Improves Feature Representation:\n",
        "\n",
        "By grouping different inflections of a word, lemmatization helps model treat them as the same concept. This can improve the quality of features extracted (e.g., for TF-IDF or word embeddings), as 'good' and 'better' are recognized as related to 'well'.\n",
        "\n",
        "Focuses on Semantic Meaning:\n",
        "\n",
        "It allows the model to focus more on the core meaning of words rather than their grammatical variations. This can be crucial for style analysis, where the semantic content might be similar but the stylistic choices differ.\n",
        "\n",
        "Potential for Better Accuracy:\n",
        "\n",
        "By normalizing word forms, lemmatization can lead to better generalization and potentially higher accuracy for classification model, especially if the differences between human and AI text are subtle and relate to core vocabulary usage."
      ],
      "metadata": {
        "id": "PWD7ktZv9R_8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8ffd77a"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e89bfac7"
      },
      "source": [
        "!pip install -U spacy\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fc70e55"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_texts(texts, batch_size=1000):\n",
        "    lemmatized = []\n",
        "    for doc in nlp.pipe(\n",
        "        texts,\n",
        "        batch_size=batch_size,\n",
        "        disable=[\"ner\", \"parser\"]\n",
        "    ):\n",
        "        lemmatized.append(\" \".join(token.lemma_ for token in doc))\n",
        "    return lemmatized"
      ],
      "metadata": {
        "id": "oKn2lWwT4gkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined['text_lemmatized'] = lemmatize_texts(df_combined['text'].tolist())"
      ],
      "metadata": {
        "id": "OXGYDV8o4oKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined"
      ],
      "metadata": {
        "id": "w2SWAFTR7Tcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6f3d85a"
      },
      "source": [
        "df_combined_lemmatized = df_combined.copy(deep=True)\n",
        "df_combined_lemmatized.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c354ec2"
      },
      "source": [
        "df_combined_lemmatized = df_combined_lemmatized.drop(columns=['text'])\n",
        "df_combined_lemmatized = df_combined_lemmatized.rename(columns={'text_lemmatized': 'text'})\n",
        "df_combined_lemmatized.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad816659"
      },
      "source": [
        "df_combined_lemmatized.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8f640bc"
      },
      "source": [
        "df_combined_lemmatized.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "408d015f"
      },
      "source": [
        "df_combined_lemmatized.to_csv('/content/drive/My Drive/Human vs AI Generated Text Classification/df_combined_lemmatized.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}