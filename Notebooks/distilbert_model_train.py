# -*- coding: utf-8 -*-
"""distilbert_model_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11oYWN1MFUeBjIlRguiNUn2H-d-mEPItk
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df_combined = pd.read_csv('/content/drive/My Drive/Human vs AI Generated Text Classification/df_combined.csv')

df_combined

pip install transformers torch scikit-learn

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification
from torch.optim import AdamW # Corrected import for AdamW
from tqdm.notebook import tqdm

# Set device to GPU if available, else CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)
model.to(device)

# Tokenize the 'text' column
encodings = tokenizer(df_combined['text'].tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')

# Prepare labels
labels = torch.tensor(df_combined['label'].tolist())

# Create a TensorDataset
dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], labels)

# Split data into training and validation sets
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size

train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

# Create DataLoaders
batch_size = 64

train_dataloader = DataLoader(train_dataset,
                              sampler = RandomSampler(train_dataset), # Select batches randomly
                              batch_size = batch_size) # Trains with this batch size

val_dataloader = DataLoader(val_dataset,
                            sampler = SequentialSampler(val_dataset), # Sequences batches for evaluation
                            batch_size = batch_size) # Evaluates with this batch size

optimizer = AdamW(model.parameters(), lr=5e-5)
epochs = 20

train_losses = []
val_accuracies = []
val_precisions = []
val_recs = []
val_f1s = []

for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in tqdm(train_dataloader, desc=f"Training Epoch {epoch+1}"):
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        model.zero_grad()

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()
        loss.backward()
        optimizer.step()

    avg_train_loss = total_loss / len(train_dataloader)
    train_losses.append(avg_train_loss)
    print(f"Average training loss: {avg_train_loss:.2f}")

    # Validation phase
    model.eval()
    predictions, true_labels = [], []
    for batch in tqdm(val_dataloader, desc=f"Validation Epoch {epoch+1}"):
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask)

        logits = outputs.logits.detach().cpu().numpy()
        label_ids = labels.to('cpu').numpy()

        predictions.extend(np.argmax(logits, axis=1).flatten())
        true_labels.extend(label_ids.flatten())

    accuracy = accuracy_score(true_labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')

    val_accuracies.append(accuracy)
    val_precisions.append(precision)
    val_recs.append(recall)
    val_f1s.append(f1)

    print(f"Validation Accuracy: {accuracy:.2f}")
    print(f"Validation Precision: {precision:.2f}")
    print(f"Validation Recall: {recall:.2f}")
    print(f"Validation F1-score: {f1:.2f}")

"""**Visualizing Training Progress**
I will visualize the training loss and validation metrics (accuracy, precision, recall, F1-score) over the training epochs. This will help us understand the model's performance and convergence during training.
"""

import matplotlib.pyplot as plt
import numpy as np

epochs_range = range(1, epochs + 1)

plt.figure(figsize=(15, 8))

metrics = [
    (val_accuracies, 'Validation Accuracy', 'Accuracy', 'green'),
    (val_precisions, 'Validation Precision', 'Precision', 'red'),
    (val_recs, 'Validation Recall', 'Recall', 'blue'),
    (val_f1s, 'Validation F1-score', 'F1-score', 'orange')
]

for i, (data, title, ylabel, color) in enumerate(metrics):
    plt.subplot(2, 2, i + 1)
    plt.plot(epochs_range, data, '-', label=title, color=color)
    plt.title(f'{title} per Epoch')
    plt.xlabel('Epoch')
    plt.ylabel(ylabel)
    plt.ylim(0.90, 1.00)
    plt.xticks(epochs_range)
    plt.legend()
    plt.grid(True)

plt.tight_layout()
plt.show()



"""**Plotting the ROC Curve**

To further evaluate the model's performance, especially its ability to distinguish between classes, we will plot the Receiver Operating Characteristic (ROC) curve and calculate the Area Under the Curve (AUC).
"""

from sklearn.metrics import roc_curve, roc_auc_score

model.eval()
all_logits = []
all_labels = []

for batch in tqdm(val_dataloader, desc="Collecting predictions for ROC curve"):
    input_ids = batch[0].to(device)
    attention_mask = batch[1].to(device)
    labels = batch[2].to(device)

    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)

    logits = outputs.logits.detach().cpu()
    all_logits.append(logits)
    all_labels.append(labels.cpu())

all_logits = torch.cat(all_logits, dim=0)
all_labels = torch.cat(all_labels, dim=0)

# Convert logits to probabilities using softmax
probabilities = torch.nn.functional.softmax(all_logits, dim=1)[:, 1].numpy() # Get probability of the positive class (label 1)
true_labels_roc = all_labels.numpy()

# Calculate ROC curve and AUC
fpr, tpr, thresholds = roc_curve(true_labels_roc, probabilities)
auc_score = roc_auc_score(true_labels_roc, probabilities)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Distilbert Model')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()



"""**Plotting the Confusion Matrix**

To understand the types of errors our model is making, we will plot a Confusion Matrix. This matrix will visually represent the number of true positive, true negative, false positive, and false negative predictions made by the model on the validation set.
"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

model.eval()
predictions = []
true_labels = []

for batch in tqdm(val_dataloader, desc="Collecting predictions for Confusion Matrix"):
    input_ids = batch[0].to(device)
    attention_mask = batch[1].to(device)
    labels = batch[2].to(device)

    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)

    logits = outputs.logits.detach().cpu().numpy()
    label_ids = labels.to('cpu').numpy()

    predictions.extend(np.argmax(logits, axis=1).flatten())
    true_labels.extend(label_ids.flatten())

# Compute the confusion matrix
cm = confusion_matrix(true_labels, predictions)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Distilbert Model')
plt.show()

"""**Saving the Trained DistilBERT Model**"""

# Define the directory to save the model and tokenizer
output_dir = '/content/drive/My Drive/Human vs AI Generated Text Classification/Distilbert/'

# Save the fine-tuned model
model.save_pretrained(output_dir)

# Save the tokenizer
tokenizer.save_pretrained(output_dir)

print(f"Model and tokenizer saved to {output_dir}")